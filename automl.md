# AutoML의 부상: 배경, 핵심 연구 분야 및 최신 동향

## 등장 배경: AutoML의 필요성과 등장 계기

머신러닝 모델 개발은 데이터 전처리, 특징 공학, 알고리즘 선택, 하이퍼파라미터 튜닝 등 여러 복잡한 단계를 거칩니다[\[1\]](https://en.wikipedia.org/wiki/Automated_machine_learning#:~:text=data%20points%20to%20be%20used,by%20the%20machine%20learning%20expert). 이러한 과정은 전문 지식과 많은 시간·자원이 요구되어, **비전문가가 접근하기 어려운 장벽**으로 작용해왔습니다. 특히 딥러닝 모델의 경우 신경망 아키텍처 설계까지 필요하므로 난이도가 더욱 높습니다[\[2\]](https://en.wikipedia.org/wiki/Automated_machine_learning#:~:text=After%20these%20steps%2C%20practitioners%20must,by%20the%20machine%20learning%20expert). AutoML(Automated Machine Learning)은 이와 같은 **모델 개발 전 과정을 자동화**하여 문제를 해결하고자 등장했습니다[\[3\]](https://en.wikipedia.org/wiki/Automated_machine_learning#:~:text=an%20artificial%20intelligence,designed%20models.%5B%204). AutoML의 목표는 **머신러닝 비전문가도 손쉽게 ML 기법을 활용**할 수 있도록 하는 것이며, 자동화를 통해 개발 시간을 단축하고 경우에 따라 인간 전문가가 수작업으로 설계한 모델보다 높은 성능의 모델을 얻는 것까지 가능하게 합니다[\[3\]](https://en.wikipedia.org/wiki/Automated_machine_learning#:~:text=an%20artificial%20intelligence,designed%20models.%5B%204). 실제로 AutoML은 복잡한 ML 파이프라인을 자동화함으로써 **간소한 솔루션을 더 빠르게 구축**하고, 숙련된 연구자들조차도 놓칠 수 있는 최적 모델을 발견해내는 사례를 보여주고 있습니다[\[3\]](https://en.wikipedia.org/wiki/Automated_machine_learning#:~:text=an%20artificial%20intelligence,designed%20models.%5B%204).

요컨대, **모델 개발의 복잡성 증가와 전문인력 부족**이라는 배경 속에서 AutoML이 부상하게 되었습니다. AutoML은 데이터 준비에서부터 모델 선정·튜닝, 그리고 배포에 이르는 과정을 알고리즘적으로 수행하여 **ML의 민주화(democratization)**를 이끌고 있습니다[\[4\]](https://arxiv.org/html/2507.05962v1#:~:text=accessibility%20challenge%20by%20attempting%20to,2021%29%2C%20and%20platforms%20like). 이는 기업이나 조직이 머신러닝의 이점을 누리면서도 전문 인력 의존도를 낮추고, ML 솔루션 구현에 소요되는 시간과 비용을 크게 절감할 수 있게 합니다. 이러한 배경에서 AutoML 연구가 활발히 전개되어 왔으며, 아래에서는 그 주요 흐름을 살펴보겠습니다.

## 주요 연구 흐름 및 대표 기법

AutoML 분야는 다양한 방향으로 연구되어 왔으며, 그 중에서도 **신경망 구조 검색(NAS)**, **하이퍼파라미터 최적화(HPO)**, **특징(feature) 공학 자동화**, **ML 파이프라인 최적화 및 메타러닝** 등이 핵심 흐름으로 자리잡았습니다. 각 분야별로 대표적인 연구와 기법을 살펴보겠습니다.

### Neural Architecture Search (NAS; 신경망 아키텍처 검색)

NAS는 **신경망 구조 자체를 자동으로 설계**하는 분야로, 딥러닝 모델의 성능을 좌우하는 네트워크 구조를 인간 대신 탐색합니다. 2017년 구글 브레인의 Zoph & Le 연구는 NAS의 시초격으로, **강화학습**을 이용한 신경망 아키텍처 검색을 제안했습니다. 이 방법에서는 RNN 기반의 컨트롤러(Controller)가 신경망 구조를 표현하는 문자열(모델 설명)을 생성하고, 해당 구조를 자식 네트워크로 학습시킨 후 성능을 **보상(reward)**으로 받아 다시 컨트롤러를 학습시키는 방식을 취했습니다[\[5\]](https://arxiv.org/abs/1611.01578#:~:text=understanding,the%20Penn%20Treebank%20dataset%2C%20our). 이렇게 훈련된 NAS 컨트롤러는 CIFAR-10 등의 데이터셋에서 인간 전문가가 고안한 최고 성능 모델과 맞먹는 신경망 구조를 **자동으로 발견**해냈고, 일부 경우 인간 모델보다 더 낮은 오류율을 달성하기도 했습니다[\[5\]](https://arxiv.org/abs/1611.01578#:~:text=understanding,the%20Penn%20Treebank%20dataset%2C%20our). 이는 NAS가 충분한 자원만 있다면 전문가 수준의 네트워크 설계를 자동화할 수 있음을 보여준 획기적인 결과였습니다.

그러나 초기 NAS 기법은 수만 개의 후보 구조를 일일이 학습하여 평가해야 하므로 **막대한 계산 비용**이 드는 문제가 있었습니다. 이를 개선하기 위한 연구로 2018년 제안된 **ENAS(Efficient NAS)** 기법이 있습니다. ENAS는 **파라미터 공유**(weight sharing) 아이디어를 도입하여, 컨트롤러가 생성한 자식 네트워크들이 **공통으로 가중치 일부를 공유**하도록 함으로써 일일이 처음부터 다시 학습하지 않아도 되게 만들었습니다[\[6\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EA%B0%95%ED%99%94%20%ED%95%99%EC%8A%B5%20%EA%B8%B0%EB%B0%98%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98,%EC%9E%AC%ED%99%9C%EC%9A%A9%EC%9D%B4%20%EC%97%86%EC%96%B4%20%EA%B3%84%EC%82%B0%20%EB%B3%B5%EC%9E%A1%EB%8F%84%20%EB%AC%B8%EC%A0%9C%EC%97%90). 이 방법을 통해 기존 NAS 대비 **검색 비용을 대폭 절감**하면서도 성능에 큰 손실 없이 효율적으로 우수한 구조를 찾을 수 있음을 보였습니다[\[6\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EA%B0%95%ED%99%94%20%ED%95%99%EC%8A%B5%20%EA%B8%B0%EB%B0%98%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98,%EC%9E%AC%ED%99%9C%EC%9A%A9%EC%9D%B4%20%EC%97%86%EC%96%B4%20%EA%B3%84%EC%82%B0%20%EB%B3%B5%EC%9E%A1%EB%8F%84%20%EB%AC%B8%EC%A0%9C%EC%97%90).

또 다른 방향으로, **미분 가능 NAS** 연구도 주요한 흐름입니다. 2018년 제안된 **DARTS(Differentiable Architecture Search)**는 NAS 문제를 연속적 공간으로 **완화(relaxation)**하여 **경사하강법**으로 최적화하는 획기적인 접근을 선보였습니다[\[7\]](https://arxiv.org/abs/1806.09055#:~:text=,differentiable%20techniques). 기존에는 신경망 구조 선택이 이산적인 문제라서 진화나 강화학습처럼 비미분 최적화에 의존했지만, DARTS는 아키텍처 선택 변수를 연속 매개변수로 두어 신경망 구조를 **실시간으로 학습**할 수 있게 했습니다[\[8\]](https://arxiv.org/abs/1806.09055#:~:text=,for%20language%20modeling%2C%20while%20being). 그 결과, CIFAR-10 등에서 높은 성능의 CNN 구조를 찾는 데 소요되는 시간이 기존 방법들보다 **수십~수백 배 이상 단축**되었고, 최종 성능도 최첨단 기법에 견주는 수준을 보였습니다[\[9\]](https://arxiv.org/abs/1806.09055#:~:text=non,differentiable%20techniques). 이처럼 **강화학습**, **진화적 알고리즘**, **미분가능 최적화** 등을 활용한 다양한 NAS 방법들이 등장하면서, 신경망 설계의 자동화가 현실화되고 있습니다. 현재 NAS 연구는 **셀 기반 구조 검색**(예: NASNet의 셀 구조 활용)으로 검색 공간을 정의하고, **검증 데이터셋**을 이용해 후보 구조의 성능을 빠르게 **추정하는 전략**을 공통적으로 채택하는 추세입니다[\[10\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EB%98%90%ED%95%9C%2C%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98%20%ED%83%90%EC%83%89%20%EA%B8%B0%EC%88%A0%EC%9D%80,30%5D%20%EA%B4%80%EC%A0%90%EC%97%90%EC%84%9C%20%EC%A0%95%ED%99%95%EB%8F%84%20%EB%8C%80%EB%B9%84).

대표 논문으로는 NAS 분야를 개척한 Zoph & Le의 논문(ICLR 2017)[\[5\]](https://arxiv.org/abs/1611.01578#:~:text=understanding,the%20Penn%20Treebank%20dataset%2C%20our), 효율성을 높인 ENAS(Pham 등, 2018)[\[6\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EA%B0%95%ED%99%94%20%ED%95%99%EC%8A%B5%20%EA%B8%B0%EB%B0%98%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98,%EC%9E%AC%ED%99%9C%EC%9A%A9%EC%9D%B4%20%EC%97%86%EC%96%B4%20%EA%B3%84%EC%82%B0%20%EB%B3%B5%EC%9E%A1%EB%8F%84%20%EB%AC%B8%EC%A0%9C%EC%97%90), 그리고 미분가능 NAS를 제시한 DARTS(ICLR 2019)[\[7\]](https://arxiv.org/abs/1806.09055#:~:text=,differentiable%20techniques) 등을 꼽을 수 있습니다. 이들 연구는 각각 NAS의 **효과성**, **효율성**, **확장성**을 크게 진전시켰습니다.

### 하이퍼파라미터 최적화 (Hyperparameter Optimization, HPO)

하이퍼파라미터 최적화는 **모델의 최적 하이퍼파라미터 설정을 자동으로 탐색**하는 분야입니다. 전통적으로 하이퍼파라미터 튜닝은 **그리드 탐색**(격자 형태로 조합을 체계적으로 시도)이나 **랜덤 탐색**에 의존해왔습니다. 하지만 그리드 탐색은 조합 수가 조금만 늘어도 연산량이 기하급수적으로 증가하고, 랜덤 탐색은 효율은 높지만 **체계적 가이드가 부족**한 한계가 있습니다[\[11\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=1)[\[12\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EC%98%88%EC%B2%98%EB%9F%BC%20%ED%95%98%EC%9D%B4%ED%8D%BC%20%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0%20%EA%B5%AC%EA%B0%84%20%EB%82%B4%EC%97%90%EC%84%9C,%EB%B0%9C%EA%B2%AC%ED%95%A0%20%EC%88%98%20%EC%9E%88%EB%8A%94%20%EA%B0%80%EB%8A%A5%EC%84%B1%EC%9D%84%20%EB%86%92%EC%98%80%EB%8B%A4).

이를 보완하기 위해 **베이지안 최적화**가 HPO에 널리 도입되었습니다. 베이지안 최적화는 현재까지의 탐색 결과를 **서로게이트 모델**(대체 모델)로 학습하여, 다음 탐색할 후보를 제안하는 **획득 함수**(acquisition function)를 통해 **효율적으로 최적해를 수렴**하는 방법입니다[\[13\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=2)[\[14\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EC%9C%84%EC%99%80%20%EA%B0%99%EC%9D%80%20%EB%B0%A9%EC%8B%9D%EC%9C%BC%EB%A1%9C%20%EB%8F%99%EC%9E%91%ED%95%98%EB%8A%94%20%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88,%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0%20%EC%B5%9C%EC%A0%81%ED%99%94%20%EA%B8%B0%EC%88%A0%EC%9D%B4%20%EB%93%B1%EC%9E%A5%ED%95%98%EA%B3%A0%20%EC%9E%88%EB%8B%A4). 2012년 Snoek 등 연구[\[14\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EC%9C%84%EC%99%80%20%EA%B0%99%EC%9D%80%20%EB%B0%A9%EC%8B%9D%EC%9C%BC%EB%A1%9C%20%EB%8F%99%EC%9E%91%ED%95%98%EB%8A%94%20%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88,%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0%20%EC%B5%9C%EC%A0%81%ED%99%94%20%EA%B8%B0%EC%88%A0%EC%9D%B4%20%EB%93%B1%EC%9E%A5%ED%95%98%EA%B3%A0%20%EC%9E%88%EB%8B%A4) 이후로 베이지안 최적화는 하이퍼파라미터 튜닝의 주류 기법이 되었는데, 복잡한 모델일수록 1회 평가 비용이 크기 때문에 순수 베이지안 최적화만으로는 시간이 많이 걸리는 경우가 많았습니다[\[15\]](https://arxiv.org/abs/1807.01774#:~:text=,consistently%20outperforms%20both%20Bayesian%20optimization). 특히 최신 딥러닝에 적용할 때 **수십 수백 번의 학습**을 해야 하는 베이지안 접근은 현실적으로 어려움이 있었고, 이를 개선하기 위한 다양한 연구가 등장했습니다.

그 중 최근 주목받은 기법이 **멀티-피델리티(multifidelity) 기반 최적화**입니다. 이는 **적은 리소스로 대략적인 성능을 평가**하여 나쁜 설정을 조기에 제거하고, 유망한 설정에만 추가 자원을 할당하는 방식입니다. 대표적으로 Li 등(2018)이 제안한 **Hyperband** 알고리즘이 있습니다. Hyperband는 일정 예산 내에서 **무작위로 여러 설정을 부분 학습**시킨 뒤, **성능이 낮은 절반을 과감히 잘라내고** 남은 절반을 더 긴 학습으로 이어가는 **서세(iterative) 방식을 반복**합니다[\[16\]](https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#:~:text=match%20at%20L2112%20A%20popular,performing%20ones%20early). 이렇게 하면 한정된 예산으로도 많은 후보를 시험해볼 수 있고, **낮은 성능의 후보를 초기에 제거**하여 리소스를 절약할 수 있습니다[\[16\]](https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#:~:text=match%20at%20L2112%20A%20popular,performing%20ones%20early). Hyperband는 기존 **Successive Halving** 기법을 확장한 것으로, 탐색 시작 시점의 후보 수와 단계별 자원 증분을 달리하는 **여러 bracket을 병렬로 수행**하여 탐색 효율을 극대화합니다[\[17\]](https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#:~:text=Hyperband%20solves%20this%20problem%20by,where)[\[18\]](https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#:~:text=An%20example%20Hyperband%20schedule%20is,hyperparameter%20configurations%3B%20note%20that%20each). 이 방법은 랜덤 탐색의 **폭넓은 탐색**과 조기중단 전략의 **자원 절약**을 결합해 **빠른 임의 성능 향상**을 이루었습니다.

또 하나의 중요한 발전은 **BOHB**입니다. BOHB(Falkner 등, 2018)는 **Bayesian Optimization과 Hyperband의 장점을 결합**한 기법으로, **베이지안 최적화의 지능적 탐색**에 **Hyperband의 속도**를 접목했습니다[\[19\]](https://arxiv.org/abs/1807.01774#:~:text=infeasible,forward%20neural%20networks%2C%20Bayesian). 구체적으로, Hyperband가 무작위로 후보를 선택하던 단계를 베이지안 최적화로 대체하여 **성능 향상 가능성이 높은 후보에 우선순위**를 두고 평가합니다[\[20\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=match%20at%20L188%20Hyperband%EB%A5%BC%20%EC%9D%B4%EC%9A%A9%ED%95%B4,%EC%8B%A4%ED%97%98%EC%97%90%EC%84%9C%20%EA%B0%80%EC%9E%A5%20%EC%9A%B0%EC%88%98%ED%95%9C%20%EC%84%B1%EB%8A%A5%EC%9D%84%20%EB%B3%B4%EC%98%80%EB%8B%A4)[\[21\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=Hyperband%EB%A5%BC%20%EC%9D%B4%EC%9A%A9%ED%95%B4%20%EB%AC%B4%EC%9E%91%EC%9C%84%EB%A1%9C%20%ED%95%98%EC%9D%B4%ED%8D%BC%20%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0,%EC%8B%A4%ED%97%98%EC%97%90%EC%84%9C%20%EA%B0%80%EC%9E%A5%20%EC%9A%B0%EC%88%98%ED%95%9C%20%EC%84%B1%EB%8A%A5%EC%9D%84%20%EB%B3%B4%EC%98%80%EB%8B%A4). BOHB는 Tree Parzen Estimator(TPE) 기반의 서러게이트 모델을 사용하여 구현의 단순성과 효율성을 높였으며, 다중 작업 병렬화까지 지원하여 32개의 워커를 사용하면 **약 15배까지 탐색 속도를 향상**시킬 수 있음을 보였습니다[\[22\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=BOHB,8%5D%EA%B0%80)[\[21\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=Hyperband%EB%A5%BC%20%EC%9D%B4%EC%9A%A9%ED%95%B4%20%EB%AC%B4%EC%9E%91%EC%9C%84%EB%A1%9C%20%ED%95%98%EC%9D%B4%ED%8D%BC%20%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0,%EC%8B%A4%ED%97%98%EC%97%90%EC%84%9C%20%EA%B0%80%EC%9E%A5%20%EC%9A%B0%EC%88%98%ED%95%9C%20%EC%84%B1%EB%8A%A5%EC%9D%84%20%EB%B3%B4%EC%98%80%EB%8B%A4). 결과적으로 BOHB는 베이지안 최적화 단독이나 Hyperband 단독보다도 **일관되게 우수한 성능**을 다양한 문제(고차원 함수, SVM, 신경망, 강화학습 등)에서 달성했습니다[\[23\]](https://arxiv.org/abs/1807.01774#:~:text=configurations%20as%20quickly,the%20same%20time%20being%20conceptually).

요약하면, HPO 분야에서는 **베이지안 최적화**를 기반으로 한 기법들이 주류를 이루면서도 **멀티-피델리티 전략(Hyperband 등)**과 **결합 기법(BOHB 등)**을 통해 **탐색 속도와 확실성**을 모두 잡는 방향으로 발전해 왔습니다. 이 외에도 진화 알고리즘, **Population Based Training(PBT)**, **PSO(입자군집화)**, 서러게이트 모델 등 다양한 접근들이 시도되었지만, 현재는 Hyperband 계열과 BOHB 계열이 실용적인 표준으로 자리잡았습니다[\[24\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=match%20at%20L364%20%EC%97%B0%EA%B5%AC%EB%93%A4%EC%9D%B4%20%EC%A0%9C%EC%95%88%EB%90%98%EA%B3%A0,%ED%92%80%EA%B3%A0%EC%9E%90%20%ED%95%98%EB%8A%94%20%EC%97%B0%EA%B5%AC%EB%93%A4%EB%8F%84%20%EC%B6%9C%ED%98%84%ED%95%98%EA%B3%A0%20%EC%9E%88%EB%8B%A4). 더 나아가 최근에는 **신경망 구조 탐색(NAS)과 하이퍼파라미터 탐색을 통합적으로 풀려는 연구**[\[25\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EC%97%B0%EA%B5%AC%EB%93%A4%EC%9D%B4%20%EC%A0%9C%EC%95%88%EB%90%98%EA%B3%A0%20%EC%9E%88%EB%8B%A4,%ED%92%80%EA%B3%A0%EC%9E%90%20%ED%95%98%EB%8A%94%20%EC%97%B0%EA%B5%AC%EB%93%A4%EB%8F%84%20%EC%B6%9C%ED%98%84%ED%95%98%EA%B3%A0%20%EC%9E%88%EB%8B%A4)도 나오고 있는데, 이는 뒤에서 다시 언급하겠습니다.

### Feature Engineering 자동화 (자동 특징 공학)

**특징 공학(feature engineering)**은 모델의 성능을 좌우하는 중요한 단계이지만, 도메인 지식과 많은 수작업을 필요로 합니다. AutoML의 한 흐름은 이 **특징 생성과 선택 과정을 자동화**하여 데이터 사이언티스트의 부담을 줄이는 것입니다. 예를 들어 **AutoFeat**(Franziska Horn 등, 2019)이라는 오픈소스 라이브러리는 **비선형 조합 특징을 자동 생성**한 뒤, 그 중 **유의미한 소수의 특징을 선택**하여 원래의 선형 모델 성능을 크게 향상시키면서도 모델의 해석 가능성은 유지할 수 있음을 보였습니다[\[26\]](https://arxiv.org/abs/1901.07329#:~:text=feature%20engineering%20and%20selection%20capabilities,meaningful%20features%20is%20selected%2C%20which). AutoFeat는 다단계 과정으로 동작하는데, 우선 주어진 입력 피처들로부터 다항식 변환, 곱셈, 로그 등 **다양한 수학적 변환을 조합**하여 방대한 후보 특징들을 만들어내고, 이어서 그중 가장 효과적인 특징들을 탐색해 최종 모델에 포함시킵니다[\[26\]](https://arxiv.org/abs/1901.07329#:~:text=feature%20engineering%20and%20selection%20capabilities,meaningful%20features%20is%20selected%2C%20which). 이를 통해 **복잡한 비선형 모델의 성능을 선형 모델로도 달성**할 수 있도록 하여, 모델을 보다 **투명하고 설명 가능**하게 만들면서도 예측 정확도를 높이는 성과를 얻었습니다[\[26\]](https://arxiv.org/abs/1901.07329#:~:text=feature%20engineering%20and%20selection%20capabilities,meaningful%20features%20is%20selected%2C%20which).

또 다른 대표적인 기법은 **딥(feature) Feature Synthesis (DFS)**입니다. DFS는 MIT에서 제안한 알고리즘으로, **관계형 데이터베이스** 내 여러 테이블에 걸쳐 **자동으로 새로운 특징을 생성**합니다[\[27\]](https://groups.csail.mit.edu/EVO-DesignOpt/groupWebSite/uploads/Site/DSAA_DSM_2015.pdf#:~:text=II,explain%20the%20motivation%20for%20Deep). 방법은 간단히 말해, **엔티티 간의 관계를 따라가면서** 수치 필드들에 대한 합계, 평균, 최대값 등의 **집계 연산**을 단계적으로 적용해 나가는 것입니다[\[27\]](https://groups.csail.mit.edu/EVO-DesignOpt/groupWebSite/uploads/Site/DSAA_DSM_2015.pdf#:~:text=II,explain%20the%20motivation%20for%20Deep). 이를 통해 "사용자가 최근 구매한 제품의 평균 가격", "해당 고객의 주문 횟수" 등과 같은 **유용한 파생 특징들**을 자동으로 만들어냅니다. DFS는 특징을 만들 때 연산을 누적적(stacking)으로 적용할 수 있어서, 한 번의 연산으로 나온 특징을 다시 다른 연산의 입력으로 사용함으로써 **특징 생성의 깊이(depth)**를 늘려갑니다[\[28\]](https://groups.csail.mit.edu/EVO-DesignOpt/groupWebSite/uploads/Site/DSAA_DSM_2015.pdf#:~:text=generates%20features%20for%20relational%20datasets,abstractions%2C%20and%20present%20the%20algorithm). 예컨대 "고객별 총 주문금액의 표준편차"와 같이 **2단계 이상의 조합 특징**도 생성해낼 수 있으며, 이러한 깊은 특징일수록 잠재적으로 더 복잡한 관계를 포착합니다. DFS는 FeatureTools 등의 오토ML 도구에 구현되어 **사람이 일일이 SQL 쿼리를 작성하지 않아도** 수백 개의 새로운 피처를 자동 생성해주는 것으로 널리 활용되고 있습니다.

**구글의 AutoML Tables**와 같은 상용 AutoML 플랫폼 또한 **특징 공학을 자동화**하는 실용 사례입니다. AutoML Tables는 구조화된 표 데이터에 대해 **자동으로 데이터 전처리와 특징 변환, 알고리즘 선택**까지 수행하여 최적 모델을 찾아주는 서비스입니다[\[29\]](https://cloud.google.com/blog/products/ai-machine-learning/new-automl-features-and-end-to-end-workflows-on-ai-platform-pipelines#:~:text=AutoML%20Tables%20%20lets%20you,46%2C%20and%20more). 사용자는 별다른 코딩 없이 데이터만 업로드하고 몇 번의 클릭으로 **학습을 시작**할 수 있으며, AutoML이 알아서 범주형 변수 인코딩, 누락값 처리, 교차 특성 생성 등을 내부적으로 수행합니다[\[30\]](https://medium.com/googledeveloperseurope/make-your-life-easier-with-automl-tables-on-google-cloud-8a745c4e7f67#:~:text=So%2C%20I%20suggested%20him%20to,results%20ready%20the%20next%20morning). 한 사례로, NASA Space Apps 해커톤에서 한 참가자가 AutoML Tables를 사용해 **코딩 없이 하룻밤만에 모델을 완성**한 일이 소개되었는데, 데이터 로드부터 학습, 평가, 그리고 배포까지 **모두 GUI와 자동화로 처리**되어 다음 날 바로 예측 결과를 얻을 수 있었다고 합니다[\[30\]](https://medium.com/googledeveloperseurope/make-your-life-easier-with-automl-tables-on-google-cloud-8a745c4e7f67#:~:text=So%2C%20I%20suggested%20him%20to,results%20ready%20the%20next%20morning). 이처럼 AutoML Tables 등의 플랫폼은 **사전 데이터 처리와 특징 엔지니어링을 내장**하고 있어, 전문지식 없이도 비교적 양질의 특징셋(feature set)을 확보할 수 있도록 도와줍니다.

### 파이프라인 최적화 및 메타러닝

AutoML의 완성형 목표는 **전체 머신러닝 파이프라인을 자동 구성**하는 것입니다. 여기에는 데이터 전처리, 특징 선택, 알고리즘(모델) 선택, 하이퍼파라미터 튜닝, 앙상블 등에 이르는 **종합적인 워크플로우의 최적화**가 포함됩니다. 이를 위해 **메타러닝(meta-learning)** 개념이 많이 활용됩니다. 메타러닝이란 **과거의 학습 경험으로부터 새로운 학습을 가속**하거나 개선하는 방법으로, AutoML에서는 **이전 데이터셋들에서 어떤 모델과 설정이 잘 작동했는지를 학습**해 새로운 데이터셋에 빠르게 적용하는 데 쓰입니다[\[31\]\[32\]](https://medium.com/@anixlynch/meta-learning-in-automl-more-accurate-than-traditional-ml-3be72215db0d#:~:text=Meta,the%20need%20for%20manual%20intervention).

대표적인 AutoML 프레임워크로 **Auto-sklearn**이 있습니다. Auto-sklearn은 scikit-learn 기반으로 개발된 오토ML 도구로, **베이지안 최적화**를 통해 다양한 학습기와 전처리 조합 중 최적 파이프라인을 찾아냅니다[\[33\]](https://www.aqsone.com/en/blog/tpot-vs-auto-sklearn-comparing-two-automl-libraries#:~:text=Introduction%20to%20Auto). 특징적인 것은 메타러닝을 도입했다는 점인데, 수백 개의 공개 데이터셋에 대해 어떤 모델구성이 좋은 성능을 냈는지 **메타-데이터베이스**를 만들어 두고, 새로운 데이터에 Auto-sklearn을 돌리면 그 메타정보를 참고하여 **초기 후보군을 똑똑하게 선택**합니다[\[34\]](https://www.aqsone.com/en/blog/tpot-vs-auto-sklearn-comparing-two-automl-libraries#:~:text=A%20second%20method%20used%20by,and%20to%20optimize%20calculation%20time). 이를 통해 쓸모없을 것이 뻔한 모델 유형이나 하이퍼파라미터 조합은 애초에 시도하지 않음으로써 **탐색 시간을 절약**하고, 적은 시도로도 우수한 결과를 얻을 확률을 높입니다[\[35\]](https://www.aqsone.com/en/blog/tpot-vs-auto-sklearn-comparing-two-automl-libraries#:~:text=Auto,is%20very%20expensive%20to%20calculate). Auto-sklearn은 또한 최종적으로 여러 상위 모델을 **앙상블**하여 안정적 성능을 내는 기능도 갖추고 있어, 2015년 NIPS AutoML 챌린지 등에서 좋은 성적을 거둔 바 있습니다.

또 다른 도구 **TPOT(Tree-based Pipeline Optimization Tool)**는 **유전 알고리즘**을 이용하여 파이프라인을 최적화합니다[\[36\]](https://www.aqsone.com/en/blog/tpot-vs-auto-sklearn-comparing-two-automl-libraries#:~:text=TPOT%20%28Tree,the%20best%20individuals%20are%20retained). TPOT은 데이터 전처리(예: 스케일링, PCA)부터 모델(예: 결정트리, SVM, XGBoost)까지 일련의 단계들을 하나의 **유전자(sequence)**로 표현하고, 진화 알고리즘의 돌연변이와 교차를 통해 세대를 거듭하며 최적 파이프라인을 “진화”시킵니다[\[36\]](https://www.aqsone.com/en/blog/tpot-vs-auto-sklearn-comparing-two-automl-libraries#:~:text=TPOT%20%28Tree,the%20best%20individuals%20are%20retained). 초기에는 무작위 파이프라인 집합으로 시작해 반복마다 성능이 나쁜 파이프라인은 도태시키고, 우수한 파이프라인을 변이·교배하여 새로운 후보를 만들면서 탐색합니다. TPOT의 장점은 **다양한 비선형 조합을 시도**해볼 수 있다는 것이며, 최종적으로 발견된 최적 파이프라인을 **파이썬 코드로까지 자동 생성**해주기 때문에 사용자가 그대로 활용할 수 있습니다[\[37\]](https://www.aqsone.com/en/blog/tpot-vs-auto-sklearn-comparing-two-automl-libraries#:~:text=TPOT%20supports%20a%20wide%20variety,and%20train%20the%20ML%20model). 다만 유전 알고리즘 특성상 반복 실행마다 결과가 달라질 수 있고, 탐색 속도가 베이지안 방법보다는 느릴 수 있다는 trade-off가 있습니다[\[38\]](https://www.aqsone.com/en/blog/tpot-vs-auto-sklearn-comparing-two-automl-libraries#:~:text=Because%20of%20the%20use%20of,time%20the%20model%20is%20trained). 그럼에도 TPOT은 사용이 간편하고(**라인 몇 줄의 코드**로 AutoML 수행), 비교적 **짧은 시간에 그럴듯한 모델을 얻기 쉬워** 초보자들에게 인기가 높습니다[\[39\]](https://www.aqsone.com/en/blog/tpot-vs-auto-sklearn-comparing-two-automl-libraries#:~:text=As%20seen%20above%2C%20TPOT%20is,it%20doesn%27t%20require%20programming%20knowledge).

**H2O AutoML**도 빼놓을 수 없습니다. H2O.ai에서 개발한 오토ML 스위트로, 여러 가지 머신러닝 알고리즘(예: XGBoost, GLM, RandomForest 등)을 **정해진 시간 내에 병렬로 모두 학습**시키고, 그 중 상위 모델들을 **스태킹 앙상블**하여 최종 모델을 만들어내는 방식입니다[\[40\]](https://medium.com/@anixlynch/meta-learning-in-automl-more-accurate-than-traditional-ml-3be72215db0d#:~:text=Besides%20Auto,learning). H2O AutoML의 특징은 별도의 복잡한 설정 없이 **주어진 시간 동안 최선을 다해 탐색**한다는 점과, 내부적으로 **과거 시도들의 결과(메타러닝)도 활용**하여 점진적으로 성능을 올린다는 점입니다[\[40\]](https://medium.com/@anixlynch/meta-learning-in-automl-more-accurate-than-traditional-ml-3be72215db0d#:~:text=Besides%20Auto,learning). 특히 H2O는 **모델간 블렌딩과 스태킹**을 자동화하여 단일 모델보다 일관적이고 높은 예측 성능을 확보하는 데 강점이 있습니다[\[40\]](https://medium.com/@anixlynch/meta-learning-in-automl-more-accurate-than-traditional-ml-3be72215db0d#:~:text=Besides%20Auto,learning). 이러한 기능으로 H2O AutoML은 캐글(Kaggle) 등 데이터 사이언스 현업에서 실용적으로 많이 쓰이고 있으며, R이나 파이썬에서 간단히 함수 호출만으로 사용할 수 있어 편의성도 높습니다.

이밖에도 **Auto-Keras**(딥러닝 모델 전용 AutoML), **Auto-PyTorch**, **AutoGluon**, **MLBox** 등 다양한 오토ML 툴킷들이 연구 및 배포되고 있습니다[\[41\]](https://arxiv.org/html/2507.05962v1#:~:text=workflows%20%28Hutter%20et%C2%A0al,complex%20configuration%20interfaces%2C%20understand%20technical)[\[40\]](https://medium.com/@anixlynch/meta-learning-in-automl-more-accurate-than-traditional-ml-3be72215db0d#:~:text=Besides%20Auto,learning). 이들 각각은 저마다의 탐색 전략(예: Auto-Keras는 신경망 블록 조합 탐색, AutoGluon은 견고한 앙상블 등)과 강점을 가지지만, 공통적으로 **파이프라인 구성 요소의 자동화**라는 큰 목표 아래 발전하고 있습니다. **메타러닝**은 이러한 파이프라인 자동화에서 핵심 개념으로, AutoML 시스템이 **“어떤 데이터셋에는 어떤 접근이 좋았다”**는 경험을 축적함으로써 **시간이 지남에 따라 똑똑해지는** 효과를 내고 있습니다[\[40\]](https://medium.com/@anixlynch/meta-learning-in-automl-more-accurate-than-traditional-ml-3be72215db0d#:~:text=Besides%20Auto,learning). 이는 AutoML이 단순한 brute-force 자동화가 아니라 **학습하는 AutoML**로 진화하고 있음을 보여줍니다.

## AutoML 연구의 핵심 개념: 탐색 공간, 탐색 알고리즘, 성능 추정 전략

_AutoML 워크플로우: 데이터 준비, 특징 공학(모델 엔지니어링), 모델 생성(탐색 공간 및 최적화 방법), 모델 평가(저비용 성능 추정 기법)까지 다양한 자동화 단계로 구성된다_[_\[42\]_](https://commons.wikimedia.org/wiki/File:AutoML_diagram.png#:~:text=English%3A%20%20This%20Figure%20illustrates,Model%20Generation%2C%20and%20Model%20Evaluation)_._ AutoML 방법론을 이해하기 위해서는 **세 가지 핵심 개념**을 짚고 넘어갈 필요가 있습니다. 첫째는 **탐색 공간(Search Space)**의 정의이고, 둘째는 그 공간을 탐색하는 **탐색 알고리즘(Search Strategy)**이며, 셋째는 후보의 우열을 가리는 **성능 추정 전략(Performance Estimation)**입니다[\[43\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EB%8B%A4%EC%9D%8C%EC%9C%BC%EB%A1%9C%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98%20%ED%83%90%EC%83%89%20%EA%B8%B0%EC%88%A0%EC%9D%80,%ED%83%90%EC%83%89%ED%95%98%EB%8A%94%20%EB%8D%B0%20%EC%B4%88%EC%A0%90%EC%9D%84%20%EB%A7%9E%EC%B6%94%EA%B3%A0%20%EC%9E%88%EB%8B%A4). 이 개념들은 앞서 소개한 각 연구 흐름(예: NAS, HPO, 등)에서 공통적으로 나타나는 중요한 요소입니다.

- **탐색 공간 정의**: 자동화 과정에서 고려하는 **설정들의 집합**을 의미합니다. 예를 들어 HPO에서는 탐색 공간이 학습률, 나무 깊이 등 **튜닝 대상 하이퍼파라미터의 범위**가 될 것이고, NAS에서는 **신경망 레이어 조합과 연결 구조들**이 탐색 공간을 이룹니다[\[44\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EB%8B%A4%EC%9D%8C%EC%9C%BC%EB%A1%9C%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98%20%ED%83%90%EC%83%89%20%EA%B8%B0%EC%88%A0%EC%9D%80,%ED%83%90%EC%83%89%ED%95%98%EB%8A%94%20%EB%8D%B0%20%EC%B4%88%EC%A0%90%EC%9D%84%20%EB%A7%9E%EC%B6%94%EA%B3%A0%20%EC%9E%88%EB%8B%A4). 파이프라인 최적화의 경우 전처리 기법, 알고리즘 종류, 하이퍼파라미터 등이 모두 탐색 공간에 포함됩니다. 탐색 공간을 어떻게 설계하느냐에 따라 AutoML의 효율성과 성능이 크게 좌우되는데, **공간을 너무 넓게 잡으면** 불필요한 조합을 많이 검사하게 되어 비효율적이고, **너무 좁게 잡으면** 최적 솔루션을 놓칠 위험이 있습니다. 따라서 도메인 지식이나 선행 연구를 바탕으로 **적절한 제약을 둔 탐색 공간**을 설정하는 것이 중요합니다[\[45\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EC%97%B0%EA%B2%B0%EA%B5%AC%EC%A1%B0%EC%99%80%20%EA%B0%80%EC%A4%91%EC%B9%98%EB%A5%BC%20%ED%83%90%EC%83%89%20%EB%8C%80%EC%83%81%EC%9C%BC%EB%A1%9C%20%EC%82%BC%EB%8A%94%EB%8B%A4,%ED%83%90%EC%83%89%ED%95%98%EB%8A%94%20%EB%8D%B0%20%EC%B4%88%EC%A0%90%EC%9D%84%20%EB%A7%9E%EC%B6%94%EA%B3%A0%20%EC%9E%88%EB%8B%A4). 예를 들어 NAS에서는 **모듈화된 셀 구조**를 기본 단위로 탐색 공간을 한정하거나, HPO에서 **로그 스케일**로 범위를 설정하는 등이 효과적인 설계로 알려져 있습니다. 탐색 공간 설계는 AutoML 연구자에게 여전히 까다로운 문제로, **검색 영역을 재설계하거나 축소**하여 효율을 높이는 연구가 최근에도 이어지고 있습니다[\[10\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EB%98%90%ED%95%9C%2C%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98%20%ED%83%90%EC%83%89%20%EA%B8%B0%EC%88%A0%EC%9D%80,30%5D%20%EA%B4%80%EC%A0%90%EC%97%90%EC%84%9C%20%EC%A0%95%ED%99%95%EB%8F%84%20%EB%8C%80%EB%B9%84).
- **탐색 알고리즘**: 정의된 탐색 공간에서 **어떤 순서와 방법으로 후보들을 선택하고 개선해나갈지**를 결정하는 전략입니다[\[43\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EB%8B%A4%EC%9D%8C%EC%9C%BC%EB%A1%9C%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98%20%ED%83%90%EC%83%89%20%EA%B8%B0%EC%88%A0%EC%9D%80,%ED%83%90%EC%83%89%ED%95%98%EB%8A%94%20%EB%8D%B0%20%EC%B4%88%EC%A0%90%EC%9D%84%20%EB%A7%9E%EC%B6%94%EA%B3%A0%20%EC%9E%88%EB%8B%A4). 간단한 예로 랜덤 탐색은 탐색 알고리즘이 무작위 추출에 불과하지만, 베이지안 최적화는 **획득함수 기반으로 다음 후보를 영리하게 선택**하는 알고리즘입니다. NAS에서는 진화 알고리즘, 강화학습, 미분기법 등이 탐색 알고리즘으로 쓰일 수 있고[\[46\]](https://medium.com/@amit25173/zero-cost-proxies-for-neural-architecture-search-1c317d757629#:~:text=%2A%20Reinforcement%20Learning,move%20forward%20in%20the%20search), 파이프라인 최적화에선 유전 알고리즘(TPOT)이나 Bayesian + 메타러닝(auto-sklearn) 등이 쓰입니다. 탐색 알고리즘의 목표는 **적은 시도로 우수한 솔루션을 찾아내는 것**입니다. 따라서 글로벌 탐색(exploration)과 로컬 세밀 탐색(exploitation)의 균형을 맞추는 것이 중요하며, 전자는 새로운 영역을 개척해 최적점을 찾도록 하고 후자는 현재 좋은 솔루션 주변을 파고들어 미세 튜닝하는 역할을 합니다[\[47\]](https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#:~:text=low,0). 현대 AutoML 연구에서는 이 균형을 위한 기법들 – 예를 들어 **MCTS(몬테카를로 트리 탐색)**, **밴디트 알고리즘**, **PBT** – 등을 도입하기도 합니다. 또한 탐색 알고리즘의 효율을 높이기 위해 **병렬화**나 **분산 처리**를 사용하는 것도 실용상 중요합니다[\[21\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=Hyperband%EB%A5%BC%20%EC%9D%B4%EC%9A%A9%ED%95%B4%20%EB%AC%B4%EC%9E%91%EC%9C%84%EB%A1%9C%20%ED%95%98%EC%9D%B4%ED%8D%BC%20%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0,%EC%8B%A4%ED%97%98%EC%97%90%EC%84%9C%20%EA%B0%80%EC%9E%A5%20%EC%9A%B0%EC%88%98%ED%95%9C%20%EC%84%B1%EB%8A%A5%EC%9D%84%20%EB%B3%B4%EC%98%80%EB%8B%A4).
- **성능 추정 전략**: 후보 솔루션(예: 특정 하이퍼파라미터 조합이나 네트워크 구조)의 **품질을 얼마나 빨리 정확히 가늠할 것인가**의 문제입니다[\[43\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EB%8B%A4%EC%9D%8C%EC%9C%BC%EB%A1%9C%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98%20%ED%83%90%EC%83%89%20%EA%B8%B0%EC%88%A0%EC%9D%80,%ED%83%90%EC%83%89%ED%95%98%EB%8A%94%20%EB%8D%B0%20%EC%B4%88%EC%A0%90%EC%9D%84%20%EB%A7%9E%EC%B6%94%EA%B3%A0%20%EC%9E%88%EB%8B%A4). 모든 후보를 **완전한 학습으로 평가**할 수 있다면 이상적이겠지만, 현실적으로 시간과 자원이 제약되므로 **빠르고 거친 평가 방법**이 필수적입니다. 가장 기본은 **검증 데이터셋을 활용한 정확도 평가**이지만, 이 역시 학습을 다 해야 나오는 값이므로 비용이 큽니다. 이를 개선하는 대표적인 전략이 앞서 언급한 **멀티-피델리티 평가**입니다. 예를 들어 Hyperband에서는 **짧은 시간(또는 작은 데이터)** 학습으로 1차 평가하고, 상위 후보만 선별해 **더 긴 시간** 학습하여 재평가하는 식으로 **계단식 평가**를 진행했습니다[\[16\]](https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#:~:text=match%20at%20L2112%20A%20popular,performing%20ones%20early). 이처럼 **학습 예산을 점증적으로 늘리면서** 유망도를 추리는 방식은 NAS에도 적용되어, **점진적으로 에폭을 늘려가며 구조를 평가**하거나 하는 기법들이 사용됩니다. 또 다른 접근은 **엔삼블링을 통한 추정**으로, 여러 작은 모델들의 결과를 합쳐서 대략의 성능을 가늠하거나, **교차검증 결과를 누적**하여 신뢰구간을 추정하는 방법도 있습니다.

최근 들어 각광받는 혁신적 방법은 **Zero-Cost Proxy**라는 개념입니다. 이는 아예 **모델을 풀로 학습시키지 않고도** 해당 모델의 잠재 성능을 예측해보는 기법들입니다[\[48\]](https://medium.com/@amit25173/zero-cost-proxies-for-neural-architecture-search-1c317d757629#:~:text=This%20is%20where%20zero,architecture%20without%20fully%20training%20it). 예를 들어 **SynFlow**, **NASWOT**, **GradNorm** 등의 지표들은 신경망의 **초기 가중치와 구조만 가지고** 그 네트워크가 얼마나 복잡한 패턴을 학습할 수 있을지 점수화합니다. Mellor 등(2020)의 선구적 연구에서는 초기 가중치로 랜덤 초기화된 네트워크에 입력을 넣었을 때 **특정 활성화 패턴을 보이는 정도**로 성능을 예측하는 기법을 제안했고, 이후 여러 zero-cost 프록시들이 개발되었습니다[\[49\]](https://iclr-blog-track.github.io/2022/03/25/zero-cost-proxies/#:~:text=Zero,The%20method). 이러한 지표들은 **신경망을 한 번도 학습시키지 않고도** 구조의 상대적 우열을 빠르게 평가해주므로, 나쁜 후보를 초기에 걸러내 탐색 효율을 극대화할 수 있습니다[\[48\]](https://medium.com/@amit25173/zero-cost-proxies-for-neural-architecture-search-1c317d757629#:~:text=This%20is%20where%20zero,architecture%20without%20fully%20training%20it)[\[50\]](https://medium.com/@amit25173/zero-cost-proxies-for-neural-architecture-search-1c317d757629#:~:text=Similarly%2C%20zero,skipping%20the%20costly%20training%20phase). 비유하자면 집을 다 지어보지 않고도 **기초 공사만 보고 집의 완성 퀄리티를 예측**하는 셈입니다[\[51\]](https://medium.com/@amit25173/zero-cost-proxies-for-neural-architecture-search-1c317d757629#:~:text=Define%20Zero). 아직 완벽하진 않지만 계속 개선되고 있으며, NAS 등에서 실제 **탐색 방향을 안내하는 성능 예측자로 활용**되고 있습니다[\[49\]](https://iclr-blog-track.github.io/2022/03/25/zero-cost-proxies/#:~:text=Zero,The%20method)[\[52\]](https://medium.com/@amit25173/zero-cost-proxies-for-neural-architecture-search-1c317d757629#:~:text=Yadav%20medium.com%20%20Zero,skipping%20the%20costly%20training%20phase).

정리하면, AutoML의 모든 방법론은 **탐색 공간을 어떻게 정의하고, 어떤 알고리즘으로 탐색하며, 어느 정도 비용으로 성능을 가늠할 것인가**라는 공통된 도전에 직면해 있습니다. 각 연구마다 이 세 축을 조합하는 방식을 달리하며 효율성과 효과를 높이고자 노력해왔습니다[\[43\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EB%8B%A4%EC%9D%8C%EC%9C%BC%EB%A1%9C%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98%20%ED%83%90%EC%83%89%20%EA%B8%B0%EC%88%A0%EC%9D%80,%ED%83%90%EC%83%89%ED%95%98%EB%8A%94%20%EB%8D%B0%20%EC%B4%88%EC%A0%90%EC%9D%84%20%EB%A7%9E%EC%B6%94%EA%B3%A0%20%EC%9E%88%EB%8B%A4). 다음 장에서는 이러한 개념들이 최근 AutoML 연구에서 어떤 방향으로 발전하고 있는지, 최신 동향을 살펴보겠습니다.

## 최근 트렌드 및 발전 방향

AutoML 분야는 빠르게 진화하고 있으며, 최근에는 **효율성 제고**와 **적용 범위 확장**을 위한 다양한 시도가 이루어지고 있습니다. 특히 멀티-피델리티 및 제로-코스트 방법, NAS의 효율화, 대규모 언어모델(LLM)과의 접목, 그리고 산업계 응용 및 클라우드 서비스화 등이 두드러진 흐름입니다.

### Multi-fidelity 및 Zero-cost Proxy 기법

**탐색 효율을 극대화**하기 위한 방법으로 멀티-피델리티(Multi-fidelity) 최적화와 제로-코스트 프록시가 활발히 연구되고 있습니다. 멀티-피델리티 기법은 앞서 설명한 Hyperband처럼 **부분적인 리소스로 빠르게 후보를 평가**한 뒤 점진적으로 정밀도를 높이는 접근입니다[\[16\]](https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#:~:text=match%20at%20L2112%20A%20popular,performing%20ones%20early). 최근에는 이 아이디어를 발전시켜, 예를 들어 **학습 에폭을 몇 단계로 나누어** 각 단계마다 성능이 나쁜 모델을 잘라내는 **동적 자원 할당** 기법이나, **데이터 샘플의 크기를 점증**시키며 튜닝하는 방법 등이 나왔습니다. 또한 Hyperband와 베이지안 최적화를 결합한 BOHB 외에도, **Freeze-Thaw BO**(학습 곡선을 예측해 조기중단 결정)나 **ASHA(Asynchronous Successive Halving)** 등 여러 변종들이 제안되어 분산 환경에서의 효율적 HPO를 구현하고 있습니다. 이들 방법은 공통적으로 **“적은 비용으로 대략 걸러내고, 좋은 놈에게만 충분한 자원을”**이라는 원칙 아래 동작하여, **탐색 시간을 기존 대비 수배 이상 단축**하는 효과를 보입니다[\[21\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=Hyperband%EB%A5%BC%20%EC%9D%B4%EC%9A%A9%ED%95%B4%20%EB%AC%B4%EC%9E%91%EC%9C%84%EB%A1%9C%20%ED%95%98%EC%9D%B4%ED%8D%BC%20%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0,%EC%8B%A4%ED%97%98%EC%97%90%EC%84%9C%20%EA%B0%80%EC%9E%A5%20%EC%9A%B0%EC%88%98%ED%95%9C%20%EC%84%B1%EB%8A%A5%EC%9D%84%20%EB%B3%B4%EC%98%80%EB%8B%A4).

**Zero-cost proxy**는 앞서 언급한 대로 **사전 학습 없이 모델 성능을 예측**하는 파격적인 방법입니다. 최근 NAS 연구에서 특히 주목받고 있는데, 대표 지표인 **NASWOT**(Without Training)을 비롯해 **SynFlow**, **Fisher**, **GradSign** 등 여러 제로코스트 점수들이 개발되었습니다[\[49\]](https://iclr-blog-track.github.io/2022/03/25/zero-cost-proxies/#:~:text=Zero,The%20method)[\[52\]](https://medium.com/@amit25173/zero-cost-proxies-for-neural-architecture-search-1c317d757629#:~:text=Yadav%20medium.com%20%20Zero,skipping%20the%20costly%20training%20phase). 예컨대 SynFlow는 신경망의 **연결 가중치 흐름을 분석**하여 **값이 0으로 소멸되지 않는 경로 수**를 세어 구조의 표현력을 평가하고, NASWOT은 **랜덤 가중치로 초기화된 신경망의 출력 다변성**(output variance)을 측정합니다. 이러한 프록시들은 일반적으로 **계산 몇 번**으로 얻을 수 있어 비용이 무시할 수준으로 적습니다. 물론 절대적인 예측 정확도는 높지 않지만, **순위 상관관계** 측면에서 어느 정도 유용성을 보여주고 있어, NAS에서 **초기 탐색 방향을 안내**하거나 HPO에서 **초벌 필터**로 활용되고 있습니다[\[48\]](https://medium.com/@amit25173/zero-cost-proxies-for-neural-architecture-search-1c317d757629#:~:text=This%20is%20where%20zero,architecture%20without%20fully%20training%20it)[\[50\]](https://medium.com/@amit25173/zero-cost-proxies-for-neural-architecture-search-1c317d757629#:~:text=Similarly%2C%20zero,skipping%20the%20costly%20training%20phase). 최근에는 여러 프록시 지표를 **앙상블하여 정확도를 높이는 연구**[\[53\]](https://arxiv.org/html/2505.09344v1#:~:text=...%20arxiv.org%20%20Zero,recent%20proxies%20often%20lack), **프록시 자체를 학습으로 진화시키는 연구(EZNAS)** 등도 진행되며 이 분야가 빠르게 발전하고 있습니다.

### NAS 효율화 및 통합 추구

신경망 아키텍처 검색(NAS) 분야는 여전히 AutoML의 핵심 연구주제로, **탐색 비용을 줄이고 실용성을 높이는 방향**으로 진화하고 있습니다. 앞서 살펴본 ENAS의 **파라미터 공유**나 DARTS의 **연속 공간 Relaxation**은 NAS 효율화를 크게 이끈 기법들입니다[\[6\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EA%B0%95%ED%99%94%20%ED%95%99%EC%8A%B5%20%EA%B8%B0%EB%B0%98%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98,%EC%9E%AC%ED%99%9C%EC%9A%A9%EC%9D%B4%20%EC%97%86%EC%96%B4%20%EA%B3%84%EC%82%B0%20%EB%B3%B5%EC%9E%A1%EB%8F%84%20%EB%AC%B8%EC%A0%9C%EC%97%90)[\[7\]](https://arxiv.org/abs/1806.09055#:~:text=,differentiable%20techniques). 여기에 더해 최근에는 **One-shot NAS**와 **Weight-sharing Supernet** 기법이 각광받았습니다. One-shot NAS는 하나의 거대 신경망(Supernet)에 모든 후보 구조를 포함시켜놓고 학습한 후, 그 **부분망을 샘플링**하여 평가하는 방법으로, ENAS와 유사하게 **모든 후보가 학습된 가중치를 일부라도 공유**하므로 탐색 속도를 비약적으로 개선합니다[\[54\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EC%A7%84%ED%99%94%20%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EB%B0%8F%20%EA%B0%95%ED%99%94%20%ED%95%99%EC%8A%B5,25). 이는 2019년구글 Brain과 CMU 등이 선보인 기법으로, 수천 GPU 시간에 달하던 NAS를 수십 GPU 시간으로 단축시켜 **NAS의 실용화 문턱을 낮추었습니다**.

또한 NAS 연구자들은 **검색 공간 자체를 영리하게 설계**하여 효율을 높이고 있습니다. 예컨대 모바일 기기를 위한 NAS에서는 **연산자 종류를 제한**하고 **모델 크기 제약**을 넣어 불필요하게 거대한 구조는 애초에 배제하거나, **모듈러 셀 구조**를 활용해 탐색 차원을 줄이는 식입니다[\[10\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EB%98%90%ED%95%9C%2C%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98%20%ED%83%90%EC%83%89%20%EA%B8%B0%EC%88%A0%EC%9D%80,30%5D%20%EA%B4%80%EC%A0%90%EC%97%90%EC%84%9C%20%EC%A0%95%ED%99%95%EB%8F%84%20%EB%8C%80%EB%B9%84). 이렇게 하면 탐색 복잡도가 감소할 뿐 아니라, **발견된 구조를 다른 문제에 전이(transfer)**하기도 용이해집니다. 실제로 NASNet에서 제안된 **셀 구조**는 이후 다양한 NAS 연구의 기본 단위로 활용되었고, 이를 통해 **ImageNet 같은 대형 데이터셋에도 NAS를 직접 적용**하는 성과를 냈습니다[\[10\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EB%98%90%ED%95%9C%2C%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98%20%ED%83%90%EC%83%89%20%EA%B8%B0%EC%88%A0%EC%9D%80,30%5D%20%EA%B4%80%EC%A0%90%EC%97%90%EC%84%9C%20%EC%A0%95%ED%99%95%EB%8F%84%20%EB%8C%80%EB%B9%84). 최근에는 **모델 경량화(quantization, pruning)** 기법과 NAS를 결합하거나, **멀티-목적 NAS**(정확도와 모델 크기/지연시간 등을 동시에 최적화) 등 통합적 접근도 시도되고 있습니다. 예를 들어 MnasNet(Tan et al., 2019)은 **모델의 모바일 CPU 지연시간**을 함께 고려하는 NAS를 수행하여, 실용적인 수준의 경량 모델을 자동으로 찾아냈습니다. 이처럼 NAS는 **효율성(Efficiency)**뿐 아니라 **현실 제약 통합(Integration)** 측면에서 발전하고 있으며, 향후에는 **HPO와 NAS를 동시에 수행하는 통합 AutoML**이 중요한 방향으로 주목받고 있습니다[\[25\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EC%97%B0%EA%B5%AC%EB%93%A4%EC%9D%B4%20%EC%A0%9C%EC%95%88%EB%90%98%EA%B3%A0%20%EC%9E%88%EB%8B%A4,%ED%92%80%EA%B3%A0%EC%9E%90%20%ED%95%98%EB%8A%94%20%EC%97%B0%EA%B5%AC%EB%93%A4%EB%8F%84%20%EC%B6%9C%ED%98%84%ED%95%98%EA%B3%A0%20%EC%9E%88%EB%8B%A4). 실제로 Auto-PyTorch나 최근 AutoML 논문들에서는 **하이퍼파라미터와 아키텍처를 하나의 최적화 문제**로 다루어 **진정한 end-to-end 자동화**를 추구하고 있습니다.

### LLM 기반 AutoML 자동화 및 파이프라인 생성

2023년 이후 눈에 띄는 트렌드는 대규모 언어 모델(LLM; 예: GPT-4)의 등장으로 **자연어 인터페이스 기반 AutoML**이 현실화되고 있다는 점입니다. 전통적인 AutoML 시스템은 여전히 사용자가 **GUI 설정**이나 **약간의 스크립팅**을 통해 구성해야 하는 부분이 있었지만[\[55\]](https://arxiv.org/html/2507.05962v1#:~:text=2020%20%29%2C%20and%20Auto,limitation%20means%20that%20even%20AutoML), 이제는 **사용자의 자연어 명령만으로** 데이터 준비부터 모델 훈련까지 진행하는 연구가 나타났습니다. 예를 들어 **AutoML-GPT**라는 실험적 시스템은 사용자가 “이런 이런 데이터로 저런 문제를 풀고 싶다”라고 **평문으로 지시하면**, LLM이 그 요구를 해석해 적절한 **데이터 전처리, 모델 선택, 하이퍼파라미터 설정, 학습 코드 작성**까지 자동으로 수행해줍니다[\[56\]](https://arxiv.org/html/2507.05962v1#:~:text=explored%20the%20integration%20of%20LLMs,focused%20mainly%20on%20technical%20automation). 즉, LLM이 **머신러닝 파이프라인을 생성**하고 제어하는 역할을 하는 것입니다. 이는 일종의 **자연어에서 AutoML로의 컴파일러**가 등장한 셈입니다.

최근 한 연구에서는 이러한 LLM 기반 AutoML 인터페이스를 통해 **비전문가 사용자들도 93% 이상에서 더 높은 정확도를 달성**했고, **개발 시간은 절반 이하로 단축**되는 등 큰 개선을 보였다고 보고했습니다[\[57\]](https://arxiv.org/html/2507.05962v1#:~:text=in%20implementing%20deep%20learning%20solutions,that%20natural%20language%20interfaces%20can). 자연어 인터페이스가 **기술 장벽을 크게 낮춰주어** ML 구현 성공률을 높이고, 사용자 만족도 역시 높았다는 분석입니다[\[57\]](https://arxiv.org/html/2507.05962v1#:~:text=in%20implementing%20deep%20learning%20solutions,that%20natural%20language%20interfaces%20can). 예를 들어, 전통적 AutoML 툴을 쓸 때는 사용자가 “분류 문제”, “정규화” 등의 용어를 알아야 했지만, LLM 기반 시스템에서는 “고객 이탈을 예측해줘”와 같은 **일상 언어**로 지시할 수 있고 시스템이 이를 이해해 적절한 솔루션을 찾습니다. 다만 아직은 한계도 있습니다. AutoML-GPT 사례에서 보고된 바에 따르면, 사용자가 **모델 선택, 평가 지표** 등의 개념을 전혀 모르면 완전히 제로코드로 쓰긴 어렵고[\[58\]](https://arxiv.org/html/2507.05962v1#:~:text=still%20has%20significant%20limitations,to%20use%20the%20system%20effectively), 기본적인 ML 용어는 알고 있어야 하는 현실적인 제한이 있습니다. 그러나 이런 방향의 연구가 활발해지면서, 장차 **음성 또는 텍스트 대화로 ML 모델을 만들어내는** 진정한 AutoML 비서가 등장할 가능성도 보입니다.

또한 LLM을 활용한 또 다른 방향은, LLM 자체를 **Optimizer**로 활용하는 것입니다. 예를 들어 **강화학습+LLM 하이브리드**로서, LLM이 현재까지의 실험 로그를 읽고 **다음 실험을 어떻게 할지 조언**해주는 형태입니다. 이는 일종의 **휴리스틱 보조**로 작용하여, AutoML 탐색을 인간 전문가처럼 인도해줄 수도 있습니다. Microsoft 등에서는 이미 대형 모델을 활용해 **자동으로 피처 추출 코드나 모델 코드를 생성**하고, 이를 곧바로 실행하여 결과를 피드백받는 **시스템 개발**을 진행 중입니다. 이러한 시도는 AutoML을 넘어 **자동 데이터 과학**(Automated Data Science)이라는 흐름과 맞닿아 있으며, 궁극적으로 AI가 AI 개발을 돕는 **자기증진적(auto-catalytic) 시스템**으로 이어질 전망입니다.

### 산업 적용 사례 및 클라우드 기반 AutoML 플랫폼

AutoML 기술은 연구실을 넘어 산업 현장에서도 빠르게 도입되고 있습니다. **구글, 아마존, MS, H2O.ai** 등에서 상용 AutoML 플랫폼을 출시하여, 개발자나 기업이 손쉽게 활용할 수 있도록 한 지 이미 몇 년 되었습니다[\[41\]](https://arxiv.org/html/2507.05962v1#:~:text=workflows%20%28Hutter%20et%C2%A0al,complex%20configuration%20interfaces%2C%20understand%20technical). 예를 들어 **Google Cloud AutoML**은 비전(이미지 분류/검출), NLP(번역, 텍스트 분류), 테이블(표 형식 데이터 예측) 등 여러 도메인에 특화된 AutoML 서비스를 제공합니다. 이를 통해 사용자들은 모델링 전문지식이 없어도 **클라우드 상에서 데이터 업로드 -> 자동 모델 학습 -> 배포**의 **엔드투엔드 ML 파이프라인**을 수행할 수 있습니다[\[30\]](https://medium.com/googledeveloperseurope/make-your-life-easier-with-automl-tables-on-google-cloud-8a745c4e7f67#:~:text=So%2C%20I%20suggested%20him%20to,results%20ready%20the%20next%20morning). **Amazon SageMaker Autopilot** 역시 tabular 데이터에 대해 자동으로 전처리, 알고리즘 선택, 하이퍼파라미터 튜닝을 해주는 서비스를 제공하며, 특히 **두 가지 모드**(다양한 알고리즘 빠르게 시도하는 모드 vs. 특정 알고리즘 정밀 튜닝 모드)를 지원하여 사용자의 요구에 맞게 AutoML을 적용할 수 있습니다[\[59\]\[59\]](https://medium.com/@bibhushabibhs/aws-automl-no-code-machine-learning-in-amazon-sagemaker-autopilot-f5d7e0be4e6a#:~:text=AWS%20AutoML%20%E2%80%94%20No%20Code,Hyper%20Parameter). 이러한 클라우드 AutoML 서비스들은 **대용량 데이터**도 분산 인프라에서 병렬 처리해주므로, 사용자는 인프라 구성이나 병렬화에 신경쓸 필요 없이 **확장성 있는 AutoML**을 활용할 수 있습니다.

산업계 활용 사례로는 **금융**에서 AutoML을 이용해 **사기 거래 탐지 모델**을 만들거나, **마케팅** 부서에서 고객 이탈 예측 모델을 AutoML로 구축하는 경우가 흔해지고 있습니다[\[29\]](https://cloud.google.com/blog/products/ai-machine-learning/new-automl-features-and-end-to-end-workflows-on-ai-platform-pipelines#:~:text=AutoML%20Tables%20%20lets%20you,46%2C%20and%20more). **의료 분야**에서도 AutoML로 빠르게 예측 모델을 만들어 전염병 유행 예측이나 환자 재입원 예측 등에 활용하는 사례가 보고됩니다. 한 연구에 따르면, AutoML 도구를 활용한 그룹이 수작업 모델링한 그룹보다 **일관되게 높은 정확도와 더 빠른 개발주기**를 보였다고 합니다[\[57\]](https://arxiv.org/html/2507.05962v1#:~:text=in%20implementing%20deep%20learning%20solutions,that%20natural%20language%20interfaces%20can). 이는 AutoML이 가져올 **생산성 혁신**을 보여주는 예로, 특히 데이터 사이언티스트 인력이 부족한 중소 조직이나, **데이터는 많은데 해석 인력이 부족한 기업**에서 AutoML에 대한 수요가 높습니다.

주요 클라우드 플랫폼들은 AutoML 서비스를 계속 진화시키고 있습니다. 구글은 AutoML을 자사의 **Vertex AI 플랫폼**에 통합하여 UI/SDK를 개선하고 있고, **Azure**도 자체 AutoML 기능을 머신러닝 스튜디오에 포함하고 있습니다[\[41\]](https://arxiv.org/html/2507.05962v1#:~:text=workflows%20%28Hutter%20et%C2%A0al,complex%20configuration%20interfaces%2C%20understand%20technical). H2O.ai의 **Driverless AI**는 AutoML을 넘어 **자동 보고서 생성, 모델 설명(Explainability)** 기능까지 제공하여 기업 현장에서 유용하게 쓰입니다. 이들 솔루션들은 대체로 **사람 전문가의 작업 흐름**(Feature Engineering -> Modeling -> Tuning -> Ensembling -> Deployment)을 모사하면서도, 반복적 작업을 자동화하고 최적화를 가속화하는 방향으로 발전 중입니다.

마지막으로, AutoML의 산업 적용에서 중요한 이슈는 **모델 해석성과 신뢰성**입니다. 자동으로 만들어진 모델이라 해도 비즈니스 현장에선 **왜 그런 예측이 나왔는지** 설명할 수 있어야 하기 때문입니다. 이에 따라 AutoML 플랫폼들은 **모델 설명 기법(SHAP, LIME 등)을 자동 적용**해주거나, **fairness(공정성) 지표**를 함께 리포팅하는 등 책임있는 AI 활용을 지원하고 있습니다. 예컨대 Google AutoML Tables는 최종 모델에 대해 **특성 중요도(feature importance)**를 제공하고, AWS Autopilot은 **모델 설명 보고서**를 자동 생성해 줍니다[\[60\]](https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development-create-experiment.html#:~:text=After%20the%20experiment%20runs%2C%20you,or%20the%20candidate%20model%20definitions)[\[61\]](https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development-create-experiment.html#:~:text=). 이는 AutoML이 **현업 표준**으로 자리잡기 위해 풀어야 할 과제이기도 합니다.

## 맺음말

AutoML은 **“AI를 만드는 AI”**로 불리며 머신러닝 분야의 지평을 넓혀왔습니다. 초기에는 학계의 흥미로운 연구 주제로 출발했지만, 이제는 다양한 **오픈소스 라이브러리와 클라우드 서비스**로 구현되어 **실제 문제 해결에 공헌**하고 있습니다. AutoML의 발전으로 머신러닝 모델링의 **진입장벽은 낮아지고**, 숙련된 연구자들도 반복적 튜닝 작업에서 해방되어 더 창의적인 문제에 집중할 수 있게 되었습니다.

물론 해결해야 할 문제도 남아 있습니다. 자동으로 생성된 모델의 **신뢰성**과 **윤리적 검증**, AutoML 프로세스의 **데이터 효율성**(적은 데이터로도 잘 작동하는지), 그리고 AutoML 시스템 간의 **표준화** 등이 그것입니다. 하지만 학계와 업계 모두 AutoML의 가치를 인정하고 있어, 앞으로도 **사람과 AI가 협업**하여 더 나은 모델을 빠르게 만드는 기술들이 속속 등장할 것으로 기대됩니다. **AI 연구자**라면 AutoML의 개념과 흐름을 폭넓게 이해하고, 이를 활용하거나 개선하는 방향으로 나아가는 것이 중요합니다. AutoML은 **머신러닝 실무의 민주화**를 이끌 뿐 아니라, 궁극적으로 **AI 연구의 자동화**라는 도전적인 목표를 향해 진화하고 있습니다. 변화의 속도가 빠른 만큼 최신 동향에 관심을 갖고 지켜볼 필요가 있으며, 오늘 소개한 내용이 AutoML 분야에 대한 폭넓은 이해에 도움이 되었기를 바랍니다.

**참고 문헌:** AutoML 개념 및 기술 동향[\[3\]](https://en.wikipedia.org/wiki/Automated_machine_learning#:~:text=an%20artificial%20intelligence,designed%20models.%5B%204)[\[1\]](https://en.wikipedia.org/wiki/Automated_machine_learning#:~:text=data%20points%20to%20be%20used,by%20the%20machine%20learning%20expert)[\[43\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EB%8B%A4%EC%9D%8C%EC%9C%BC%EB%A1%9C%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98%20%ED%83%90%EC%83%89%20%EA%B8%B0%EC%88%A0%EC%9D%80,%ED%83%90%EC%83%89%ED%95%98%EB%8A%94%20%EB%8D%B0%20%EC%B4%88%EC%A0%90%EC%9D%84%20%EB%A7%9E%EC%B6%94%EA%B3%A0%20%EC%9E%88%EB%8B%A4), NAS 분야 연구[\[5\]](https://arxiv.org/abs/1611.01578#:~:text=understanding,the%20Penn%20Treebank%20dataset%2C%20our)[\[6\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EA%B0%95%ED%99%94%20%ED%95%99%EC%8A%B5%20%EA%B8%B0%EB%B0%98%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98,%EC%9E%AC%ED%99%9C%EC%9A%A9%EC%9D%B4%20%EC%97%86%EC%96%B4%20%EA%B3%84%EC%82%B0%20%EB%B3%B5%EC%9E%A1%EB%8F%84%20%EB%AC%B8%EC%A0%9C%EC%97%90)[\[7\]](https://arxiv.org/abs/1806.09055#:~:text=,differentiable%20techniques), HPO 기법 발전[\[16\]](https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#:~:text=match%20at%20L2112%20A%20popular,performing%20ones%20early)[\[23\]](https://arxiv.org/abs/1807.01774#:~:text=configurations%20as%20quickly,the%20same%20time%20being%20conceptually), 특징 공학 자동화 사례[\[26\]](https://arxiv.org/abs/1901.07329#:~:text=feature%20engineering%20and%20selection%20capabilities,meaningful%20features%20is%20selected%2C%20which)[\[27\]](https://groups.csail.mit.edu/EVO-DesignOpt/groupWebSite/uploads/Site/DSAA_DSM_2015.pdf#:~:text=II,explain%20the%20motivation%20for%20Deep), 파이프라인 AutoML 도구[\[35\]](https://www.aqsone.com/en/blog/tpot-vs-auto-sklearn-comparing-two-automl-libraries#:~:text=Auto,is%20very%20expensive%20to%20calculate)[\[40\]](https://medium.com/@anixlynch/meta-learning-in-automl-more-accurate-than-traditional-ml-3be72215db0d#:~:text=Besides%20Auto,learning), LLM 접목 AutoML 전망[\[56\]](https://arxiv.org/html/2507.05962v1#:~:text=explored%20the%20integration%20of%20LLMs,focused%20mainly%20on%20technical%20automation)[\[57\]](https://arxiv.org/html/2507.05962v1#:~:text=in%20implementing%20deep%20learning%20solutions,that%20natural%20language%20interfaces%20can), AutoML 산업 적용[\[41\]](https://arxiv.org/html/2507.05962v1#:~:text=workflows%20%28Hutter%20et%C2%A0al,complex%20configuration%20interfaces%2C%20understand%20technical)[\[29\]](https://cloud.google.com/blog/products/ai-machine-learning/new-automl-features-and-end-to-end-workflows-on-ai-platform-pipelines#:~:text=AutoML%20Tables%20%20lets%20you,46%2C%20and%20more) 등.

[\[1\]](https://en.wikipedia.org/wiki/Automated_machine_learning#:~:text=data%20points%20to%20be%20used,by%20the%20machine%20learning%20expert) [\[2\]](https://en.wikipedia.org/wiki/Automated_machine_learning#:~:text=After%20these%20steps%2C%20practitioners%20must,by%20the%20machine%20learning%20expert) [\[3\]](https://en.wikipedia.org/wiki/Automated_machine_learning#:~:text=an%20artificial%20intelligence,designed%20models.%5B%204) Automated machine learning - Wikipedia

<https://en.wikipedia.org/wiki/Automated_machine_learning>

[\[4\]](https://arxiv.org/html/2507.05962v1#:~:text=accessibility%20challenge%20by%20attempting%20to,2021%29%2C%20and%20platforms%20like) [\[41\]](https://arxiv.org/html/2507.05962v1#:~:text=workflows%20%28Hutter%20et%C2%A0al,complex%20configuration%20interfaces%2C%20understand%20technical) [\[55\]](https://arxiv.org/html/2507.05962v1#:~:text=2020%20%29%2C%20and%20Auto,limitation%20means%20that%20even%20AutoML) [\[56\]](https://arxiv.org/html/2507.05962v1#:~:text=explored%20the%20integration%20of%20LLMs,focused%20mainly%20on%20technical%20automation) [\[57\]](https://arxiv.org/html/2507.05962v1#:~:text=in%20implementing%20deep%20learning%20solutions,that%20natural%20language%20interfaces%20can) [\[58\]](https://arxiv.org/html/2507.05962v1#:~:text=still%20has%20significant%20limitations,to%20use%20the%20system%20effectively) Evaluation of Large Language Model-Driven AutoML in Data and Model Management from Human-Centered Perspective

<https://arxiv.org/html/2507.05962v1>

[\[5\]](https://arxiv.org/abs/1611.01578#:~:text=understanding,the%20Penn%20Treebank%20dataset%2C%20our) \[1611.01578\] Neural Architecture Search with Reinforcement Learning

<https://arxiv.org/abs/1611.01578>

[\[6\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EA%B0%95%ED%99%94%20%ED%95%99%EC%8A%B5%20%EA%B8%B0%EB%B0%98%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98,%EC%9E%AC%ED%99%9C%EC%9A%A9%EC%9D%B4%20%EC%97%86%EC%96%B4%20%EA%B3%84%EC%82%B0%20%EB%B3%B5%EC%9E%A1%EB%8F%84%20%EB%AC%B8%EC%A0%9C%EC%97%90) [\[10\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EB%98%90%ED%95%9C%2C%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98%20%ED%83%90%EC%83%89%20%EA%B8%B0%EC%88%A0%EC%9D%80,30%5D%20%EA%B4%80%EC%A0%90%EC%97%90%EC%84%9C%20%EC%A0%95%ED%99%95%EB%8F%84%20%EB%8C%80%EB%B9%84) [\[11\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=1) [\[12\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EC%98%88%EC%B2%98%EB%9F%BC%20%ED%95%98%EC%9D%B4%ED%8D%BC%20%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0%20%EA%B5%AC%EA%B0%84%20%EB%82%B4%EC%97%90%EC%84%9C,%EB%B0%9C%EA%B2%AC%ED%95%A0%20%EC%88%98%20%EC%9E%88%EB%8A%94%20%EA%B0%80%EB%8A%A5%EC%84%B1%EC%9D%84%20%EB%86%92%EC%98%80%EB%8B%A4) [\[13\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=2) [\[14\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EC%9C%84%EC%99%80%20%EA%B0%99%EC%9D%80%20%EB%B0%A9%EC%8B%9D%EC%9C%BC%EB%A1%9C%20%EB%8F%99%EC%9E%91%ED%95%98%EB%8A%94%20%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88,%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0%20%EC%B5%9C%EC%A0%81%ED%99%94%20%EA%B8%B0%EC%88%A0%EC%9D%B4%20%EB%93%B1%EC%9E%A5%ED%95%98%EA%B3%A0%20%EC%9E%88%EB%8B%A4) [\[20\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=match%20at%20L188%20Hyperband%EB%A5%BC%20%EC%9D%B4%EC%9A%A9%ED%95%B4,%EC%8B%A4%ED%97%98%EC%97%90%EC%84%9C%20%EA%B0%80%EC%9E%A5%20%EC%9A%B0%EC%88%98%ED%95%9C%20%EC%84%B1%EB%8A%A5%EC%9D%84%20%EB%B3%B4%EC%98%80%EB%8B%A4) [\[21\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=Hyperband%EB%A5%BC%20%EC%9D%B4%EC%9A%A9%ED%95%B4%20%EB%AC%B4%EC%9E%91%EC%9C%84%EB%A1%9C%20%ED%95%98%EC%9D%B4%ED%8D%BC%20%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0,%EC%8B%A4%ED%97%98%EC%97%90%EC%84%9C%20%EA%B0%80%EC%9E%A5%20%EC%9A%B0%EC%88%98%ED%95%9C%20%EC%84%B1%EB%8A%A5%EC%9D%84%20%EB%B3%B4%EC%98%80%EB%8B%A4) [\[22\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=BOHB,8%5D%EA%B0%80) [\[24\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=match%20at%20L364%20%EC%97%B0%EA%B5%AC%EB%93%A4%EC%9D%B4%20%EC%A0%9C%EC%95%88%EB%90%98%EA%B3%A0,%ED%92%80%EA%B3%A0%EC%9E%90%20%ED%95%98%EB%8A%94%20%EC%97%B0%EA%B5%AC%EB%93%A4%EB%8F%84%20%EC%B6%9C%ED%98%84%ED%95%98%EA%B3%A0%20%EC%9E%88%EB%8B%A4) [\[25\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EC%97%B0%EA%B5%AC%EB%93%A4%EC%9D%B4%20%EC%A0%9C%EC%95%88%EB%90%98%EA%B3%A0%20%EC%9E%88%EB%8B%A4,%ED%92%80%EA%B3%A0%EC%9E%90%20%ED%95%98%EB%8A%94%20%EC%97%B0%EA%B5%AC%EB%93%A4%EB%8F%84%20%EC%B6%9C%ED%98%84%ED%95%98%EA%B3%A0%20%EC%9E%88%EB%8B%A4) [\[43\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EB%8B%A4%EC%9D%8C%EC%9C%BC%EB%A1%9C%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98%20%ED%83%90%EC%83%89%20%EA%B8%B0%EC%88%A0%EC%9D%80,%ED%83%90%EC%83%89%ED%95%98%EB%8A%94%20%EB%8D%B0%20%EC%B4%88%EC%A0%90%EC%9D%84%20%EB%A7%9E%EC%B6%94%EA%B3%A0%20%EC%9E%88%EB%8B%A4) [\[44\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EB%8B%A4%EC%9D%8C%EC%9C%BC%EB%A1%9C%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98%20%ED%83%90%EC%83%89%20%EA%B8%B0%EC%88%A0%EC%9D%80,%ED%83%90%EC%83%89%ED%95%98%EB%8A%94%20%EB%8D%B0%20%EC%B4%88%EC%A0%90%EC%9D%84%20%EB%A7%9E%EC%B6%94%EA%B3%A0%20%EC%9E%88%EB%8B%A4) [\[45\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EC%97%B0%EA%B2%B0%EA%B5%AC%EC%A1%B0%EC%99%80%20%EA%B0%80%EC%A4%91%EC%B9%98%EB%A5%BC%20%ED%83%90%EC%83%89%20%EB%8C%80%EC%83%81%EC%9C%BC%EB%A1%9C%20%EC%82%BC%EB%8A%94%EB%8B%A4,%ED%83%90%EC%83%89%ED%95%98%EB%8A%94%20%EB%8D%B0%20%EC%B4%88%EC%A0%90%EC%9D%84%20%EB%A7%9E%EC%B6%94%EA%B3%A0%20%EC%9E%88%EB%8B%A4) [\[54\]](https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html#:~:text=%EC%A7%84%ED%99%94%20%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%20%EB%B0%8F%20%EA%B0%95%ED%99%94%20%ED%95%99%EC%8A%B5,25) 자동 기계학습(AutoML) 기술 동향

<https://ettrends.etri.re.kr/ettrends/178/0905178004/0905178004.html>

[\[7\]](https://arxiv.org/abs/1806.09055#:~:text=,differentiable%20techniques) [\[8\]](https://arxiv.org/abs/1806.09055#:~:text=,for%20language%20modeling%2C%20while%20being) [\[9\]](https://arxiv.org/abs/1806.09055#:~:text=non,differentiable%20techniques) \[1806.09055\] DARTS: Differentiable Architecture Search

<https://arxiv.org/abs/1806.09055>

[\[15\]](https://arxiv.org/abs/1807.01774#:~:text=,consistently%20outperforms%20both%20Bayesian%20optimization) [\[19\]](https://arxiv.org/abs/1807.01774#:~:text=infeasible,forward%20neural%20networks%2C%20Bayesian) [\[23\]](https://arxiv.org/abs/1807.01774#:~:text=configurations%20as%20quickly,the%20same%20time%20being%20conceptually) \[1807.01774\] BOHB: Robust and Efficient Hyperparameter Optimization at Scale

<https://arxiv.org/abs/1807.01774>

[\[16\]](https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#:~:text=match%20at%20L2112%20A%20popular,performing%20ones%20early) [\[17\]](https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#:~:text=Hyperband%20solves%20this%20problem%20by,where) [\[18\]](https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#:~:text=An%20example%20Hyperband%20schedule%20is,hyperparameter%20configurations%3B%20note%20that%20each) [\[47\]](https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#:~:text=low,0) 5  Advanced Tuning Methods and Black Box Optimization – Applied Machine Learning Using mlr3 in R

<https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html>

[\[26\]](https://arxiv.org/abs/1901.07329#:~:text=feature%20engineering%20and%20selection%20capabilities,meaningful%20features%20is%20selected%2C%20which) \[1901.07329\] The autofeat Python Library for Automated Feature Engineering and Selection

<https://arxiv.org/abs/1901.07329>

[\[27\]](https://groups.csail.mit.edu/EVO-DesignOpt/groupWebSite/uploads/Site/DSAA_DSM_2015.pdf#:~:text=II,explain%20the%20motivation%20for%20Deep) [\[28\]](https://groups.csail.mit.edu/EVO-DesignOpt/groupWebSite/uploads/Site/DSAA_DSM_2015.pdf#:~:text=generates%20features%20for%20relational%20datasets,abstractions%2C%20and%20present%20the%20algorithm) groups.csail.mit.edu

<https://groups.csail.mit.edu/EVO-DesignOpt/groupWebSite/uploads/Site/DSAA_DSM_2015.pdf>

[\[29\]](https://cloud.google.com/blog/products/ai-machine-learning/new-automl-features-and-end-to-end-workflows-on-ai-platform-pipelines#:~:text=AutoML%20Tables%20%20lets%20you,46%2C%20and%20more) New AutoML features and end-to-end workflows on AI Platform Pipelines | Google Cloud Blog

<https://cloud.google.com/blog/products/ai-machine-learning/new-automl-features-and-end-to-end-workflows-on-ai-platform-pipelines>

[\[30\]](https://medium.com/googledeveloperseurope/make-your-life-easier-with-automl-tables-on-google-cloud-8a745c4e7f67#:~:text=So%2C%20I%20suggested%20him%20to,results%20ready%20the%20next%20morning) Make your life easier with AutoML Tables on Google Cloud! | by Ilias Papachristos | Google for Developers EMEA | Medium

<https://medium.com/googledeveloperseurope/make-your-life-easier-with-automl-tables-on-google-cloud-8a745c4e7f67>

[\[31\]](https://medium.com/@anixlynch/meta-learning-in-automl-more-accurate-than-traditional-ml-3be72215db0d#:~:text=Meta,the%20need%20for%20manual%20intervention) [\[32\]](https://medium.com/@anixlynch/meta-learning-in-automl-more-accurate-than-traditional-ml-3be72215db0d#:~:text=Meta,the%20need%20for%20manual%20intervention) [\[40\]](https://medium.com/@anixlynch/meta-learning-in-automl-more-accurate-than-traditional-ml-3be72215db0d#:~:text=Besides%20Auto,learning) Meta-Learning in AutoML: More Accurate Than Traditional ML? | by Anix Lynch, MBA, ex-VC | Medium

<https://medium.com/@anixlynch/meta-learning-in-automl-more-accurate-than-traditional-ml-3be72215db0d>

[\[33\]](https://www.aqsone.com/en/blog/tpot-vs-auto-sklearn-comparing-two-automl-libraries#:~:text=Introduction%20to%20Auto) [\[34\]](https://www.aqsone.com/en/blog/tpot-vs-auto-sklearn-comparing-two-automl-libraries#:~:text=A%20second%20method%20used%20by,and%20to%20optimize%20calculation%20time) [\[35\]](https://www.aqsone.com/en/blog/tpot-vs-auto-sklearn-comparing-two-automl-libraries#:~:text=Auto,is%20very%20expensive%20to%20calculate) [\[36\]](https://www.aqsone.com/en/blog/tpot-vs-auto-sklearn-comparing-two-automl-libraries#:~:text=TPOT%20%28Tree,the%20best%20individuals%20are%20retained) [\[37\]](https://www.aqsone.com/en/blog/tpot-vs-auto-sklearn-comparing-two-automl-libraries#:~:text=TPOT%20supports%20a%20wide%20variety,and%20train%20the%20ML%20model) [\[38\]](https://www.aqsone.com/en/blog/tpot-vs-auto-sklearn-comparing-two-automl-libraries#:~:text=Because%20of%20the%20use%20of,time%20the%20model%20is%20trained) [\[39\]](https://www.aqsone.com/en/blog/tpot-vs-auto-sklearn-comparing-two-automl-libraries#:~:text=As%20seen%20above%2C%20TPOT%20is,it%20doesn%27t%20require%20programming%20knowledge) TPOT vs Auto-sklearn: comparing two AutoML libraries | Aqsone

<https://www.aqsone.com/en/blog/tpot-vs-auto-sklearn-comparing-two-automl-libraries>

[\[42\]](https://commons.wikimedia.org/wiki/File:AutoML_diagram.png#:~:text=English%3A%20%20This%20Figure%20illustrates,Model%20Generation%2C%20and%20Model%20Evaluation) File:AutoML diagram.png - Wikimedia Commons

<https://commons.wikimedia.org/wiki/File:AutoML_diagram.png>

[\[46\]](https://medium.com/@amit25173/zero-cost-proxies-for-neural-architecture-search-1c317d757629#:~:text=%2A%20Reinforcement%20Learning,move%20forward%20in%20the%20search) [\[48\]](https://medium.com/@amit25173/zero-cost-proxies-for-neural-architecture-search-1c317d757629#:~:text=This%20is%20where%20zero,architecture%20without%20fully%20training%20it) [\[50\]](https://medium.com/@amit25173/zero-cost-proxies-for-neural-architecture-search-1c317d757629#:~:text=Similarly%2C%20zero,skipping%20the%20costly%20training%20phase) [\[51\]](https://medium.com/@amit25173/zero-cost-proxies-for-neural-architecture-search-1c317d757629#:~:text=Define%20Zero) [\[52\]](https://medium.com/@amit25173/zero-cost-proxies-for-neural-architecture-search-1c317d757629#:~:text=Yadav%20medium.com%20%20Zero,skipping%20the%20costly%20training%20phase) Zero-Cost Proxies for Neural Architecture Search | by Amit Yadav | Medium

<https://medium.com/@amit25173/zero-cost-proxies-for-neural-architecture-search-1c317d757629>

[\[49\]](https://iclr-blog-track.github.io/2022/03/25/zero-cost-proxies/#:~:text=Zero,The%20method) A Deeper Look at Zero-Cost Proxies for Lightweight NAS

<https://iclr-blog-track.github.io/2022/03/25/zero-cost-proxies/>

[\[53\]](https://arxiv.org/html/2505.09344v1#:~:text=...%20arxiv.org%20%20Zero,recent%20proxies%20often%20lack) Ensembling Zero-Cost Proxies to Estimate Performance of Neural ...

<https://arxiv.org/html/2505.09344v1>

[\[59\]](https://medium.com/@bibhushabibhs/aws-automl-no-code-machine-learning-in-amazon-sagemaker-autopilot-f5d7e0be4e6a#:~:text=AWS%20AutoML%20%E2%80%94%20No%20Code,Hyper%20Parameter) AWS AutoML — No Code Machine Learning in Amazon Sagemaker ...

<https://medium.com/@bibhushabibhs/aws-automl-no-code-machine-learning-in-amazon-sagemaker-autopilot-f5d7e0be4e6a>

[\[60\]](https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development-create-experiment.html#:~:text=After%20the%20experiment%20runs%2C%20you,or%20the%20candidate%20model%20definitions) [\[61\]](https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development-create-experiment.html#:~:text=) Create Regression or Classification Jobs for Tabular Data Using the AutoML API - Amazon SageMaker AI

<https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development-create-experiment.html>