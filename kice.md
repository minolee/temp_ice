# AI μ§λ¬΄ ν‰κ°€ λ€λΉ„ κ°•μ μλ£ (κΈ°μ  λΈ”λ΅κ·Έ μ¤νƒ€μΌ)  
## 1. λ°μ΄ν„° μ „μ²λ¦¬  
### λ°μ΄ν„° μμ§‘κ³Ό νΈν–¥ (Bias)  
- **μ‹ν— ν¬μΈνΈ:** λ°μ΄ν„° μμ§‘ μ‹ νΈν–¥ μµμ†ν™”κ°€ μ¤‘μ”ν•©λ‹λ‹¤. ν• λ…μ μ „λ¬Έκ°€λ§ λ μ΄λΈ”λ§ν•λ©΄ μΌκ΄€μ„±μ€ μƒκΈ°μ§€λ§ κ°μΈμ  νΈν–¥μ΄ μ „μ²΄ λ°μ΄ν„°μ…‹μ— λ°μλ©λ‹λ‹¤.  
- **μ‹¤λ¬΄ μ‚¬λ΅€:** ImageNetλ„ μ΄κΈ°μ—λ” μ„κµ¬κ¶ λ°μ΄ν„° μ„μ£ΌλΌμ„ μΈμΆ…Β·λ¬Έν™”μ  νΈν–¥ λ¬Έμ κ°€ λ…Όμλμ—μµλ‹λ‹¤. Googleμ β€Inclusive Images Challenge(2018)β€λ” λ‹¤μ–‘ν• μ§€μ—­ λ°μ΄ν„°λ¥Ό μ¶”κ°€ν•΄ μ΄λ¥Ό μ™„ν™”ν•κ³ μ ν–μµλ‹λ‹¤.  
- **μ—°κµ¬ μ°Έκ³ :**  
- Torralba & Efros, *Unbiased Look at Dataset Bias* (CVPR 2011).  
- Mitchell et al., *Model Cards for Model Reporting* (FAT* 2019).  
### λ°μ΄ν„° μ •μ  (Normalization vs. Standardization)  
- **μ‹ν— ν¬μΈνΈ:**  
- Min-Max Normalization: \[0,1\] κµ¬κ°„μΌλ΅ μ¤μΌ€μΌλ§ β†’ μ΄μƒμΉ(outlier)μ— λ―Όκ°.  
- Standardization: ν‰κ·  0, ν‘μ¤€νΈμ°¨ 1λ΅ λ³€ν™ β†’ νκ·€, PCAμ— μ ν•©.  
- **μ—°κµ¬/μ‚¬λ΅€:**  
- LeCun et al. (1998) MNIST β†’ μ…λ ¥ λ°μ΄ν„° μ •κ·ν™”λ¥Ό ν†µν•΄ λΉ λ¥Έ ν•™μµ λ‹¬μ„±.  
- Ioffe & Szegedy, *Batch Normalization* (ICML 2015).  
### λ°μ΄ν„° μ¦κ°• (Data Augmentation)  
- **μ‹ν— ν¬μΈνΈ:** μμ¨μ£Όν–‰ Semantic Segmentationμ—λ” Random Erasingμ΄ μ ν•©. μ΄λ” κ°€λ ¤μ§ μƒν™©μ„ ν•™μµμ‹μΌ λ¨λΈμ„ λ” κ°•μΈν•κ² λ§λ“­λ‹λ‹¤.  
- **μ—°κµ¬/μ‚¬λ΅€:**  
- Zhong et al., *Random Erasing Data Augmentation* (AAAI 2020).  
- CutMix, Mixup, AugMix λ“±μ€ λ¶„λ¥ κ³Όμ μ— κ°•μ .  
---  
## 2. AI λ¨λΈ κ°λ°  
### μ•„ν‚¤ν…μ² μ„¤κ³„ & Self-Supervised Learning  
- **μ‹ν— ν¬μΈνΈ:** SimCLR, BYOL, MAE, RotNet λ“± self-supervised κΈ°λ²•μ ν•µμ‹¬ μ΄ν•΄.  
- **μ—°κµ¬/μ‚¬λ΅€:**  
- Chen et al., *SimCLR* (ICML 2020).  
- Grill et al., *BYOL* (NeurIPS 2020).  
- He et al., *MAE* (CVPR 2022).  
### Explainable AI (XAI)  
- **μ‹ν— ν¬μΈνΈ:** CAM vs. Grad-CAM κµ¬λ¶„. CAMμ€ FC layer κ°€μ¤‘μΉ κΈ°λ°, Grad-CAMμ€ μ†μ‹¤ κΈ°μΈκΈ° κΈ°λ°.  
- **μ—°κµ¬/μ‚¬λ΅€:**  
- Zhou et al., *CAM* (CVPR 2016).  
- Selvaraju et al., *Grad-CAM* (ICCV 2017).  
### λ¨λΈ ν•™μµ & ν‰κ°€ (Learning Curve, μ§€ν‘)  
- **μ‹ν— ν¬μΈνΈ:**  
- μ–Έλ”ν”Όν…: λ¨λΈ λ³µμ΅λ„ β†‘, νΉμ§• β†‘ ν•„μ”.  
- κ³Όμ ν•©: Dropout, L2 μ •κ·ν™”, BatchNorm λ“± ν™μ©.  
- μ§€ν‘: Recall = μ•μ „-critical ν™κ²½μ—μ„ μ¤‘μ”.  
- **μ—°κµ¬/μ‚¬λ΅€:**  
- Srivastava et al., *Dropout* (JMLR 2014).  
- Powers, *Evaluation: Precision, Recall, F-measure* (2011).  
### λ¨λΈ νλ‹ (HPO & Class Imbalance)  
- **μ‹ν— ν¬μΈνΈ:**  
- Bayesian Optimization: ν¨μ¨μ  HPO.  
- ν΄λμ¤ λ¶κ· ν• β†’ SMOTE, μ–Έλ”μƒν”λ§, Focal Loss λ“±.  
- **μ—°κµ¬/μ‚¬λ΅€:**  
- Bergstra & Bengio, *Random Search for Hyper-Parameter Optimization* (JMLR 2012).  
- Lin et al., *Focal Loss* (ICCV 2017).  
---  
## 3. AI μ‹μ¤ν… κµ¬μ¶•  
### ML Pipeline & λ°°ν¬ μ „λµ  
- **μ‹ν— ν¬μΈνΈ:** Model-in-service vs. Model-as-service λΉ„κµ.  
- In-service: κΈ°μ΅΄ μΈν”„λΌ μ¬ν™μ©, μ„λ²„ λ¦¬μ†μ¤ μ μ β†‘.  
- As-service: ν™•μ¥μ„±β†‘, λ…λ¦½μ  κ΄€λ¦¬ μ©μ΄.  
- **μ‚¬λ΅€:** Google TFX, Kubeflow / Docker-K8s κΈ°λ° CI/CD.  
### MLOps & μλ™ν™”  
- **μ‹ν— ν¬μΈνΈ:** MLOps maturity level: μλ™(Level 0) β†” μλ™ν™”(Level 1+).  
- **μ‚¬λ΅€/λ°±μ„:**  
- Google Cloud, *Continuous Training for ML* (2020).  
- AWS Sagemaker, Azure ML docs.  
### λ¨λΈ μµμ ν™”  
- **μ‹ν— ν¬μΈνΈ:** λ¨λΈ κ²½λ‰ν™” κΈ°λ²• β†’ Pruning, Quantization, EfficientNetμ Compound Scaling.  
- **μ—°κµ¬/μ‚¬λ΅€:**  
- Han et al., *Deep Compression* (ICLR 2016).  
- Tan & Le, *EfficientNet* (ICML 2019).  
---  
## 4. μ£Όμ” AI νΈλ λ“  
### Zero-shot & Generalized Zero-shot Learning  
- **μ‹ν— ν¬μΈνΈ:** μƒλ΅μ΄ ν΄λμ¤ μΈμ‹ λ¥λ ¥.  
- **μ—°κµ¬/μ‚¬λ΅€:**  
- Xian et al., *Zero-shot Learning β€“ A Comprehensive Evaluation* (TPAMI 2018).  
- Radford et al., *CLIP* (ICML 2021).  
### Chain-of-Thought Prompting  
- **μ‹ν— ν¬μΈνΈ:** λ‹¨κ³„μ  μ¶”λ΅  μ λ„.  
- **μ—°κµ¬/μ‚¬λ΅€:**  
- Wei et al., *Chain-of-Thought Prompting* (NeurIPS 2022).  
### NAS & DARTS  
- **μ‹ν— ν¬μΈνΈ:** Neural Architecture Search, λ―Έλ¶„ κ°€λ¥ νƒμƒ‰(DARTS).  
- **μ—°κµ¬/μ‚¬λ΅€:**  
- Zoph & Le, *Neural Architecture Search* (ICLR 2017).  
- Liu et al., *DARTS* (ICLR 2019).  
---  
π”Ή Level 0: μλ™ μ΄μ (Manual)

λ¨λΈ κ°λ°, λ°°ν¬, μ¬ν•™μµμ„ μ‚¬λμ΄ μ§μ ‘ μ²λ¦¬.

λ°μ΄ν„° μ¤€λΉ„ β†’ ν•™μµ β†’ ν‰κ°€ β†’ λ°°ν¬κ°€ λ¨λ‘ λ‹¨λ°μ„± ν”„λ΅μ νΈ μ„±κ²©.

μ¬ν„μ„± λ¶€μ΅±, μλ™ν™” μ—†μ.

μ¤νƒ€νΈμ—…Β·μ—°κµ¬ ν”„λ΅μ νΈ μ΄κΈ° λ‹¨κ³„μ—μ„ ν”ν λ‚νƒ€λ‚¨.

π”Ή Level 1: μλ™ν™”λ νμ΄ν”„λΌμΈ (ML Pipeline Automation)

λ°μ΄ν„° μ „μ²λ¦¬, ν•™μµ, ν‰κ°€, λ°°ν¬ λ‹¨κ³„λ¥Ό CI/CD νμ΄ν”„λΌμΈμ²λΌ μ—°κ²°.

μ½”λ“ λ³€κ²½ β†’ μλ™ μ¬ν•™μµ λ° λ°°ν¬ κ°€λ¥.

Kubeflow, TFX, MLflow κ°™μ€ λ„κµ¬ ν™μ©.

ν•µμ‹¬: λ¨λΈ κ°λ°λ¶€ν„° λ°°ν¬κΉμ§€μ λ°λ³µμ„ μλ™ν™”.

π”Ή Level 2: μλ™ λ¨λ‹ν„°λ§ λ° μ¬ν•™μµ (Continuous Training / MLOps)

μ΄μ μ¤‘ λ¨λΈμ„ μ‹¤μ‹κ°„ λ¨λ‹ν„°λ§.

μ„±λ¥ μ§€ν‘, λ°μ΄ν„° λ¶„ν¬, drift νƒμ§€.

μ΄μƒ λ°μƒ μ‹ μλ™μΌλ΅ μ¬ν•™μµ νμ΄ν”„λΌμΈ μ‹¤ν–‰.

λ°μ΄ν„° λ“λ¦¬ν”„νΈΒ·κ°λ… λ“λ¦¬ν”„νΈ λ€μ‘ κ°€λ¥.

λ¨λΈ λ μ§€μ¤νΈλ¦¬μ™€ λ²„μ „ κ΄€λ¦¬ μ²΄κ³„ ν¬ν•¨.

π”Ή Level 3: μ™„μ „ μλ™ν™”λ ML μ‹μ¤ν… (Full MLOps / AutoMLOps)

λ°μ΄ν„° μμ§‘ β†’ ν•™μµ β†’ λ°°ν¬ β†’ λ¨λ‹ν„°λ§ β†’ μ¬ν•™μµκΉμ§€ μ—”λ“-ν¬-μ—”λ“ μλ™ν™”.

μΈμ  κ°μ… μµμ†ν™”, μ§€μ†μ μΈ λ¨λΈ κ°μ„ .

AutoML κΈ°λ²•κ³Ό κ²°ν•©λμ–΄, μƒλ΅μ΄ νƒμ¤ν¬μ— μλ™μΌλ΅ μ μ‘ κ°€λ¥.

λ€κ·λ¨ κΈ°μ—… ν™κ²½μ—μ„ λ©ν‘λ΅ μ‚Όλ” μµμΆ… λ‹¨κ³„.

π“ μ •λ¦¬:

Level 0 = μλ™, ad-hoc

Level 1 = ν•™μµ/λ°°ν¬ μλ™ν™” (νμ΄ν”„λΌμΈν™”)

Level 2 = λ¨λ‹ν„°λ§Β·μ¬ν•™μµ μλ™ν™” (μ§€μ†μ  μ΄μ)

Level 3 = μ™„μ „ μλ™ν™”, AutoMLκ³Ό κ²°ν•©

[μ°Έκ³ ](https://velog.io/@leesjpr/MLOps-%EC%88%98%EC%A4%80)